{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "train_data = pd.read_csv('train.csv',index_col=0,delimiter=';')\n",
    "test_data = pd.read_csv('test.csv',index_col=0,delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the columns to have an underscore between the column names\n",
    "train_columns = train_data.columns\n",
    "for col in train_columns:\n",
    "    train_data = train_data.rename(\n",
    "        columns={\n",
    "            # strip out parentheses, and their contents\n",
    "            col: re.sub(r'\\(.*', '', col)\n",
    "            .strip() \n",
    "            .replace(' ', '_')\n",
    "            .replace('-', '_') \n",
    "            .lower()  # lowercase the column name\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration on a chosen subset\n",
    "data_subset_1 = train_data[['national_park', 'elevation', 'aspect', 'slope','horizontal_distance_to_water', 'vertical_distance_to_water','horizontal_distance_to_road', 'light_at_9am', 'light_at_noon','light_at_3pm', 'horizontal_distance_to_fire_ignition_point','forest_type']]\n",
    "missing_values_1= data_subset_1.isna().sum() # Finding the missing values in the subset\n",
    "data_description_1 = data_subset_1.describe() # Numerical explenation of the data set\n",
    "data_column_dtypes_1 = data_subset_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset_2 = train_data[['soil_1','soil_2', 'soil_3', 'soil_4', 'soil_5', 'soil_6', 'soil_7', 'soil_8','soil_9', 'soil_10', 'soil_11', 'soil_12', 'soil_13', 'soil_14','soil_15', 'soil_16', 'soil_17', 'soil_18', 'soil_19', 'soil_20','soil_21', 'soil_22', 'soil_23', 'soil_24', 'soil_25', 'soil_26','soil_27', 'soil_28', 'soil_29', 'soil_30', 'soil_31', 'soil_32','soil_33', 'soil_34', 'soil_35', 'soil_36', 'soil_37', 'soil_38','soil_39', 'soil_40', 'forest_type']]\n",
    "missing_values_2 = data_subset_2.isna().sum() # Finding the missing values in the subset\n",
    "data_description_2 = data_subset_2.describe() # Numerical explenation of the data set\n",
    "data_column_dtypes_2 = data_subset_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mount Rainer' 'Yosemite' 'Yellowstone' 'Acadia']\n",
      "['Cottonwood' 'Lodgepole']\n",
      "Lodgepole     872095\n",
      "Cottonwood    526000\n",
      "Name: forest_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data_subset_1['national_park'].unique())\n",
    "print(data_subset_1['forest_type'].unique())\n",
    "print(data_subset_2['forest_type'].value_counts())\n",
    "#print(missing_values_1)\n",
    "#print(missing_values_2)\n",
    "#data_description_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we clean and force the data to be numerical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = train_data.copy()\n",
    "df = df.replace(to_replace= ['Mount Rainer','Yosemite','Yellowstone','Acadia','Cottonwood','Lodgepole'], \n",
    "                value= [0,1,0,2,0,1] ) # Cotttenwood = 0 and Lodgepole = 1\n",
    "\n",
    "x = df.iloc[:,:-1].copy()\n",
    "y = df.iloc[:,-1].copy()\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.4,random_state=100,stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piplines for classification algorithms logistic regression, possibly regularized, Support Vector Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy L1 Logistic Regression: 0.94489287\n",
      "Test Accuracy L1 Logistic Regression: 0.94534527\n",
      "\n",
      "Train Accuracy L2 Logistic Regression: 0.94488572\n",
      "Test Accuracy L2 Logistic Regression: 0.94534349\n"
     ]
    }
   ],
   "source": [
    "# Using regularization methods L1 and L2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "x_lr_train,y_lr_train,x_lr_test,y_lr_test= x_train.copy(),y_train.copy(),x_test.copy(),y_test.copy()\n",
    "\n",
    "lr_pipe_l1 = make_pipeline(StandardScaler(),LogisticRegression(random_state=1,solver='liblinear',penalty='l1',C = 0.5))\n",
    "lr_pipe_l1.fit(x_lr_train, y_lr_train)\n",
    "lr_pipe_l2 = make_pipeline(StandardScaler(),LogisticRegression(random_state=1,solver='liblinear',penalty='l2', C = 0.5))\n",
    "lr_pipe_l2.fit(x_lr_train, y_lr_train)\n",
    "\n",
    "print('Test Accuracy L1 Logistic Regression: %.8f' % lr_pipe_l1.score(x_lr_train, y_lr_train))\n",
    "print('Test Accuracy L1 Logistic Regression: %.8f' % lr_pipe_l1.score(x_lr_test, y_lr_test))\n",
    "print()\n",
    "print('Train Accuracy L2 Logistic Regression: %.8f' % lr_pipe_l2.score(x_lr_train, y_lr_train))\n",
    "print('Test Accuracy L2 Logistic Regression: %.8f' % lr_pipe_l2.score(x_lr_test, y_lr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy Random Forest : 0.99995\n",
      "Test accuracy Random Forest : 0.97234\n"
     ]
    }
   ],
   "source": [
    "# Pipelines for ensamble methods\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "x_ensamble_train,y_ensamble_train,x_ensamble_test,y_ensamble_test= x_train.copy(),y_train.copy(),x_test.copy(),y_test.copy()\n",
    "RF = make_pipeline(RandomForestClassifier(n_estimators=40, random_state=10,n_jobs=-1))\n",
    "RF.fit(x_ensamble_train, y_ensamble_train)\n",
    "\n",
    "print('Train accuracy Random Forest : {0:.5f}'.format(RF.score(x_ensamble_train, y_ensamble_train)))\n",
    "print('Test accuracy Random Forest : {0:.5f}'.format(RF.score(x_ensamble_test, y_ensamble_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train accuracy Random Forest : 0.99995\n",
      "Test accuracy Random Forest : 0.97234\n",
      "\n",
      "Test Accuracy L1 Logistic Regression: 0.945\n",
      "Test Accuracy L1 Logistic Regression: 0.945\n",
      "\n",
      "Train Accuracy L2 Logistic Regression: 0.945\n",
      "Test Accuracy L2 Logistic Regression: 0.945\n"
     ]
    }
   ],
   "source": [
    "#print('Train accuracy svm: {0:.5f}'.format(svm.score(X_train_sc, y_train)))\n",
    "#print('Test accuracy svm: {0:.5f}'.format(svm.score(X_test_sc, y_test)))\n",
    "print()\n",
    "print('Train accuracy Random Forest : {0:.5f}'.format(RF.score(x_ensamble_train, y_train)))\n",
    "print('Test accuracy Random Forest : {0:.5f}'.format(RF.score(x_ensamble_test, y_test)))\n",
    "print()\n",
    "print('Test Accuracy L1 Logistic Regression: %.3f' % lr_pipe_l1.score(x_lr_test, y_lr_test))\n",
    "print('Test Accuracy L1 Logistic Regression: %.3f' % lr_pipe_l1.score(x_lr_test, y_lr_test))\n",
    "print()\n",
    "print('Train Accuracy L2 Logistic Regression: %.3f' % lr_pipe_l2.score(x_lr_train, y_lr_train))\n",
    "print('Test Accuracy L2 Logistic Regression: %.3f' % lr_pipe_l2.score(x_lr_test, y_lr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Keras\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras import models\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keras= train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras1_data = df_keras.copy()\n",
    "keras1_data = pd.get_dummies(data = keras1_data, columns=['national_park'])\n",
    "keras1_data['forest_type'] = np.where(keras1_data['forest_type'] == 'Cottonwood', 0, 1)\n",
    "keras1_data = keras1_data.drop(['national_park_Yellowstone'], axis=1)\n",
    "\n",
    "x_keras1= keras1_data.drop(['forest_type'],axis = 1)\n",
    "y_keras1= keras1_data['forest_type']\n",
    "keras1_x_train, keras1_x_test, keras1_y_train, keras1_y_test = train_test_split(x_keras1,y_keras1, test_size = 0.4, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['elevation', 'aspect', 'slope', 'horizontal_distance_to_water',\n",
       "       'vertical_distance_to_water', 'horizontal_distance_to_road',\n",
       "       'light_at_9am', 'light_at_noon', 'light_at_3pm',\n",
       "       'horizontal_distance_to_fire_ignition_point', 'soil_1', 'soil_2',\n",
       "       'soil_3', 'soil_4', 'soil_5', 'soil_6', 'soil_7', 'soil_8', 'soil_9',\n",
       "       'soil_10', 'soil_11', 'soil_12', 'soil_13', 'soil_14', 'soil_15',\n",
       "       'soil_16', 'soil_17', 'soil_18', 'soil_19', 'soil_20', 'soil_21',\n",
       "       'soil_22', 'soil_23', 'soil_24', 'soil_25', 'soil_26', 'soil_27',\n",
       "       'soil_28', 'soil_29', 'soil_30', 'soil_31', 'soil_32', 'soil_33',\n",
       "       'soil_34', 'soil_35', 'soil_36', 'soil_37', 'soil_38', 'soil_39',\n",
       "       'soil_40', 'forest_type', 'national_park_Acadia',\n",
       "       'national_park_Mount Rainer', 'national_park_Yellowstone',\n",
       "       'national_park_Yosemite'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras1_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(838857, 51)\n",
      "(838857,)\n",
      "(559238, 51)\n",
      "(559238,)\n",
      "(838857, 54)\n",
      "(838857,)\n",
      "(559238, 54)\n",
      "(559238,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(keras1_x_train.shape)\n",
    "print(keras1_y_train.shape)\n",
    "print(keras1_x_test.shape)\n",
    "print(keras1_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_418 (Dense)            (None, 20)                1080      \n",
      "_________________________________________________________________\n",
      "dense_419 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_420 (Dense)            (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_421 (Dense)            (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_422 (Dense)            (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_423 (Dense)            (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_424 (Dense)            (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_425 (Dense)            (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_426 (Dense)            (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_427 (Dense)            (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_428 (Dense)            (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_429 (Dense)            (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_430 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 3,671\n",
      "Trainable params: 3,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 838857 samples, validate on 559238 samples\n",
      "Epoch 1/10\n",
      "838857/838857 [==============================] - 9s 11us/step - loss: 0.4163 - accuracy: 0.8032 - val_loss: 0.2775 - val_accuracy: 0.8841\n",
      "Epoch 2/10\n",
      "838857/838857 [==============================] - 8s 10us/step - loss: 0.1983 - accuracy: 0.9226 - val_loss: 0.1474 - val_accuracy: 0.9474\n",
      "Epoch 3/10\n",
      "838857/838857 [==============================] - 8s 10us/step - loss: 0.1498 - accuracy: 0.9412 - val_loss: 0.1632 - val_accuracy: 0.9286\n",
      "Epoch 4/10\n",
      "838857/838857 [==============================] - 8s 10us/step - loss: 0.1310 - accuracy: 0.9466 - val_loss: 0.1126 - val_accuracy: 0.9563\n",
      "Epoch 5/10\n",
      "838857/838857 [==============================] - 8s 10us/step - loss: 0.1219 - accuracy: 0.9493 - val_loss: 0.1102 - val_accuracy: 0.9543\n",
      "Epoch 6/10\n",
      "838857/838857 [==============================] - 8s 10us/step - loss: 0.1146 - accuracy: 0.9525 - val_loss: 0.1146 - val_accuracy: 0.9532\n",
      "Epoch 7/10\n",
      "838857/838857 [==============================] - 8s 10us/step - loss: 0.1121 - accuracy: 0.9532 - val_loss: 0.1139 - val_accuracy: 0.9499\n",
      "Epoch 8/10\n",
      "838857/838857 [==============================] - 8s 10us/step - loss: 0.1076 - accuracy: 0.9550 - val_loss: 0.1099 - val_accuracy: 0.9527\n",
      "Epoch 9/10\n",
      "838857/838857 [==============================] - 8s 10us/step - loss: 0.1038 - accuracy: 0.9563 - val_loss: 0.1014 - val_accuracy: 0.9577\n",
      "Epoch 10/10\n",
      "838857/838857 [==============================] - 9s 10us/step - loss: 0.0995 - accuracy: 0.9585 - val_loss: 0.1044 - val_accuracy: 0.9551\n"
     ]
    }
   ],
   "source": [
    "# Code for creating and training a ANN with Keras\n",
    "\n",
    "# 1\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(20, input_dim = 53, activation = 'relu', use_bias=True))\n",
    "model.add(Dense(20, activation = \"relu\"))\n",
    "model.add(Dense(10, activation = \"relu\"))\n",
    "model.add(Dense(20, activation = \"relu\"))\n",
    "model.add(Dense(10, activation = \"relu\"))\n",
    "model.add(Dense(20, activation = \"relu\"))\n",
    "model.add(Dense(10, activation = \"relu\"))\n",
    "model.add(Dense(20, activation = \"relu\"))\n",
    "model.add(Dense(10, activation = \"relu\"))\n",
    "model.add(Dense(20, activation = \"relu\"))\n",
    "model.add(Dense(10, activation = \"relu\"))\n",
    "model.add(Dense(20, activation = \"relu\"))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()\n",
    "\n",
    "# model compile\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'Adamax', metrics = ['accuracy'])\n",
    "\n",
    "hist = model.fit(keras1_x_train, keras1_y_train, \n",
    "                 epochs = 10, \n",
    "                 batch_size = 512,\n",
    "                 validation_data = (keras1_x_test, keras1_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras2_data = df_keras.copy()\n",
    "keras2_data = pd.get_dummies(data = keras2_data.copy(), columns=['national_park'])\n",
    "keras2_data['forest_type'] = np.where(keras2_data['forest_type'] == 'Cottonwood', 0, 1)\n",
    "\n",
    "#keras2_data = keras2_data.replace(to_replace= ['Cottonwood','Lodgepole'], \n",
    "#                value= [0,1] ) # Cotttenwood = 0 and Lodgepole = 1\n",
    "\n",
    "\n",
    "x_keras2= keras2_data.drop(['forest_type'],axis = 1)\n",
    "y_keras2= keras2_data['forest_type']\n",
    "\n",
    "keras2_x_train,keras2_x_test,keras2_y_train,keras2_y_test = train_test_split(x_keras2,y_keras2,test_size=0.4,random_state=100,stratify=y_keras2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for creating and training a ANN with Keras\n",
    "\n",
    "# 2\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Dense(20, input_dim = 54, activation = 'relu'))\n",
    "model1.add(Dense(10, activation = \"relu\"))\n",
    "model1.add(Dense(20, activation = \"relu\"))\n",
    "model1.add(Dense(10, activation = \"relu\"))\n",
    "model1.add(Dense(1, activation = \"relu\"))\n",
    "model1.summary()\n",
    "\n",
    "# model compile\n",
    "\n",
    "model1.compile(loss = 'binary_crossentropy', optimizer = 'Adamax', metrics = ['accuracy'])\n",
    "\n",
    "hist = model1.fit(keras2_x_train, keras2_y_train, \n",
    "                 epochs = 20, \n",
    "                 batch_size = 128,\n",
    "                 validation_data = (keras2_x_test, keras2_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = test_data.copy()\n",
    "submission_data = pd.get_dummies(data = train_data.copy(), columns=['national_park'])\n",
    "submission_data['forest_type'] = np.where(train_data['forest_type'] == 'Cottonwood', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data['forest_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub= submission_data.iloc[:,:-1].copy()\n",
    "y_sub= submission_data.iloc[:,-1].copy()\n",
    "sub_x_train, sub_x_test, sub_y_train, sub_y_test = train_test_split(X_sub,y_sub, test_size = 0.4, random_state=100, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim = 54, activation = 'relu'))\n",
    "model.add(Dense(10, activation = \"relu\"))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()\n",
    "\n",
    "# model compile\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "hist = model.fit(sub_x_train, sub_y_train, \n",
    "                 epochs = 10, \n",
    "                 batch_size = 128,\n",
    "                 validation_data = (sub_x_test, sub_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_predictions = model.predict(submission_data)\n",
    "output = pd.DataFrame({'index': submission_data.index,'Predicted': keras_predictions})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Jan 17 2023, 16:39:35) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70c27028200b630fba820c4b92e789c342190faabea5b4273641962ed5c6a988"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
